 In my two last studies, I covered the physically motivated statistical method to extract cosmological information from the large scale structure statistics (LSS), given that we already obtained the statistics from simulation or from observation. In observation, our main concern is obtaining statistics for the LSS. Since 80% of the matter in the universe is dark matter, which is unobservable through electromagnetic waves, a good way to map the matter distribution in observation is using weak gravitational lensing: when light from distant galaxies travels to earth, its trajectory is bent by massive objects (other galaxy structure or dark matter halos, we call them foreground mass) along its way, this would distort the shape (size and ellipticity) of the distant galaxies in observation. Assuming the physical ellipticities of close by galaxies at distance has no intrinsic correlation (and thus, has average ellipticity of 0), traveling along similarly distributed foreground would make the observed average ellipticity of this group of galaxies to deviate from 0. We can then get information about the foreground by measuring the ordered statistics of observed galaxies ellipticities as a function of solid angle. Say the most distant observed galaxies are at distance 10, and the earth is at 0.  Galaxies ellipticities at 1 tell us cumulative information about all the matters between 0 and 1, and Galaxies ellipticities at 5 tell us cumulative information about all the matters between 0 and 5. Here comes another question: how do we translate the ordered statistics (to be specific, spherical power spectrum) of galaxies ellipticities into 3D distribution of the foreground? If we could only observe their final signal when arriving on earth, how can we get all the detailed structure along their trajectory traveling to earth? We can do this by observing our universe layer by layer like piling onions. We want to find a way to “chop” the continuous space between 0 and 10 into discrete intervals for observational purposes. In observation, data are taken discretely by snapshots. For each snapshot, there’s limited spatial coverage and range of depth that the snapshot can cover. When planning an observation, we want to strategically plan the snapshots we take so that the observation is not only able to tell us about galaxies ellipticities, but also give us more information from the data compared to what we already obtained in the past. My job for this research is to design an optimization algorithm so that the LSST telescope can automatically find the optimal schedule (ways to chop the space) for the next weak lensing observational run based on the data obtained from all the previous observational runs. The telescope is planning to start taking data in 2023, and we want to ensure the forecasting strategies for the upcoming schedules are reliable. Therefore, we first generated a bunch of fake signals to simulate the forecasting process. Since we want to find the optimal schedule for the next run based on previous runs, we use Maximum Likelihood Estimation (MLE) where the priors Theta_i with their confidence intervals are cosmological parameters obtained from previous runs, and the observable Chi is the spherical power spectrum based on the prior plus the “chop of the space” of the observational run. We calculate the mathematical form of the fisher information matrix of Chi with respect to theta, which measures the amount of information about theta that is carried in Chi. Then, we start a round of observational runs with the “chop” that maximize the fisher information, obtaining the event Chi_i from the runs. We then infer the newly estimated theta_i+1 from i, and check the consistency between i and i+1. We repeat this process of constructing the model based on MLE, maximizing fisher information matrix, calculating the “chop”, launching the observational run, obtaining and checking the statistics until the fisher information matrix is sufficiently small. 
